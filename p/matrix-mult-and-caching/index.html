<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><link rel=icon type=image/ico href=https://bnmoch3.org/favicon.ico?><link rel=icon type=image/png sizes=16x16 href=https://bnmoch3.org/favicon-16x16.png?><link rel=icon type=image/png sizes=32x32 href=https://bnmoch3.org/favicon-32x32.png?><link rel=icon type=image/png sizes=192x192 href=https://bnmoch3.org/android-chrome-192x192.png?><link rel=apple-touch-icon sizes=180x180 href=https://bnmoch3.org/apple-touch-icon.png?><meta name=description content><title>Optimizing CPU & Memory Interaction: Matrix Multiplication | bnmoch3
</title><link rel=canonical href=https://bnmoch3.org/p/matrix-mult-and-caching/><meta property="og:url" content="https://bnmoch3.org/p/matrix-mult-and-caching/"><meta property="og:site_name" content="bnmoch3"><meta property="og:title" content="Optimizing CPU & Memory Interaction: Matrix Multiplication"><meta property="og:description" content="Same algo, different memory access patterns, what could go wrong (or right)!"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-07T00:00:00+00:00"><meta property="article:modified_time" content="2024-01-07T00:00:00+00:00"><meta property="article:tag" content="Computer Systems"><link rel=stylesheet href=/assets/combined.min.01980ad4202828eb32272e7b1654f79f3c0022c15b1c932668dff73dffaf7e88.css media=all></head><body class=light><div class=content><header><div class=header><div class=flex><p class=small><a href=/>/home</a></p><p class=small><a href=/about>/about</a></p><p class=small><a href=/posts>/posts</a></p><p class=small><a href=/notes>/notes</a></p><p class=small><a href=/tags>/tags</a></p></div></div></header><main class=main><div class=breadcrumbs><a href=/>Home</a>
<span class=breadcrumbs-separator>> </span><a href=/posts/>Posts</a>
<span class=breadcrumbs-separator>> </span><a class=breadcrumbs-current href=/p/matrix-mult-and-caching/>Optimizing CPU & Memory Interaction: Matrix Multiplication</a></div><div><div class=single-intro-container><h1 class=single-title>Optimizing CPU & Memory Interaction: Matrix Multiplication</h1><p class=single-readtime><time datetime=2024-01-07T00:00:00+00:00>January 7, 2024</time>
&nbsp; Â· &nbsp;
14 min read</p></div><div class=single-content><h2 class=heading id=introduction>Introduction
<a href=#introduction>#</a></h2><p>Matrix multiplication is a classic example for demonstrating how memory access
patterns significantly impact an algorithm&rsquo;s runtime. The <code>O(N^3)</code> algorithm
remains unchanged but by optimizing the way memory accessed, we can improve the
runtime by as much as 40x.</p><h2 class=heading id=straightforward-algorithm>Straightforward algorithm
<a href=#straightforward-algorithm>#</a></h2><p>We&rsquo;ll be focusing on NxN (square) matrices where N is assumed to be very large.</p><p>Matrix multiplication involves taking two matrices A and B and combining them
into a new matrix C.</p><p><figure><div><img loading=lazy alt="matrix multiplication diagram" src=/p/matrix-mult-and-caching/images/matrix_multiplication_diagram.png width=330px height=290px></div></figure><em>credits wikipedia</em></p><p>For each element in C, we take the dot product of the associated row in A with
the associated column in B. The mathematical definition from wikipedia is
definitely clearer and more precise:</p><p><figure><div><img loading=lazy alt="matrix multiplication formula" src=/p/matrix-mult-and-caching/images/matrix_multiplication_formula.svg></div></figure><em>credits wikipedia</em></p><p>This definition also lends itself to the following straightforward algorithm
[1]:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#007020>#define N 2
</span></span></span><span style=display:flex><span><span style=color:#007020></span><span style=color:#007020;font-weight:700>typedef</span> <span style=color:#902000>double</span> matrix[N][N];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#902000>void</span> <span style=color:#06287e>multiply</span>(matrix A, matrix B, matrix C, <span style=color:#902000>int</span> n) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>int</span> i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> n; i<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>int</span> j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> n; j<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>            <span style=color:#902000>double</span> res <span style=color:#666>=</span> <span style=color:#40a070>0.0</span>;
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>int</span> k <span style=color:#666>=</span> <span style=color:#40a070>0</span>; k <span style=color:#666>&lt;</span> n; k<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>                res <span style=color:#666>+=</span> A[i][k] <span style=color:#666>*</span> B[k][j];
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>            C[i][j] <span style=color:#666>=</span> res;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#902000>int</span> <span style=color:#06287e>main</span>() {
</span></span><span style=display:flex><span>    matrix A <span style=color:#666>=</span> { { <span style=color:#40a070>1</span>, <span style=color:#40a070>2</span>}, { <span style=color:#40a070>3</span>, <span style=color:#40a070>4</span>} };
</span></span><span style=display:flex><span>    matrix B <span style=color:#666>=</span> { { <span style=color:#40a070>5</span>, <span style=color:#40a070>6</span>}, { <span style=color:#40a070>7</span>, <span style=color:#40a070>8</span>} };
</span></span><span style=display:flex><span>    matrix C <span style=color:#666>=</span> { <span style=color:#40a070>0</span> };
</span></span><span style=display:flex><span>    <span style=color:#06287e>multiply</span>(A, B, C, N);
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// C: [ 19.0  22.0
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#60a0b0;font-style:italic>//      43.0  50.0 ]
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#007020;font-weight:700>return</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>A couple of points worth pointing out from the above code snippet:</p><ul><li>Its complexity is <code>O(N^3)</code> where N is the length/width of the matrix. For each
entry in C, we do N amount of work, there are N^2 entries, thus N^3.</li><li>Every entry per source element (A & B) is read N times</li></ul><p>The goal of this post is to detail how the straightforward approach can be
optimized with regards to memory accesses. Before going any further, we&rsquo;ll need
to introduce caching in computer systems, plus some key terms and definitions:</p><h2 class=heading id=caches--locality>Caches & Locality
<a href=#caches--locality>#</a></h2><p>A basic computer system consists of the CPU and a memory system for data
storage. Transferring data back and forth from memory is quite slow from the
CPU&rsquo;s perspective. Therefore, in practice, a cache is added in between the CPU
and memory to speed up access and bridge the processor-memory gap.</p><p>Note, in this discussion, I&rsquo;ll treat the cache as a unified component but if you
dig further in [1], the cache is in fact composed of multiple caches (L1, L2,
L3) each organized hierarchically with the higher caches (the ones closer to the
CPU) holding a smaller subset of the data compared to the lower ones.</p><p>The cache&rsquo;s function is twofold: keep frequently accessed data close to the CPU
and keep neighbouring data close by. This strategy leverages the concept of
<strong>locality</strong>.</p><p>Programs usually exhibit two kinds of locality that contribute to the
effectiveness of caching:</p><ul><li><strong>temporal locality</strong>: once a program references a memory location, it&rsquo;s
likely to reference it again multiple times in the near future [1].</li><li><strong>spatial locality</strong>: once a program references references a memory location,
it&rsquo;s likekly to reference nearby locations in the near future [1].</li></ul><p>Thus, caching the contents of a memory address plus the adjacent data into a
singular block that&rsquo;s stored in a cache line advances both temporal and spatial
locality.</p><h2 class=heading id=caches-reads-and-writes>Caches, Reads and Writes
<a href=#caches-reads-and-writes>#</a></h2><p>With caches in the picture, reads and writes carried out by the CPU become a bit
more complicated.</p><p>When the CPU performs a read for a word (the data unit referenced by an
address), the word&rsquo;s associated block is first checked in the the cache. If the
block is present, we get a <strong>cache hit</strong>. However, if absent, we get a <strong>cache
miss</strong>: its block has to be fetched from memory, then the address&rsquo;s contents
loaded into a register in the CPU.</p><p>As detailed in [1], writes are a bit more complex. There are two cases:</p><ul><li><strong>write hit</strong>: the address being written to is already in the cache as a
block. When writing to it, there are two approaches:<ul><li><strong>write-through</strong>: upon writing to the cached block, also update its
underlying block in the memory. Advantages: You can evict a cache line
without having to write it to memory. Also, in a multi-core setup, it
guarantess consistency trivially. Disadvantage: every write results in a
block transfer back to memory; this increases traffic/decreases relative
performance.</li><li><strong>write-back</strong>: defer updating the underlying block in memory until
necessary (such as when evicting the block from the cache). Meanwhile, the
block in memory remains stale. Advantage: decreases traffic by exploiting
locality (we might write to the same location soon or to nearby locations).
Disadvantage: Implementation is more complex - in a multi-core setup, we now
require a cache coherency protocol to maintain consistency [3].
Additionally, some evictions will result in writes to memory [3].</li></ul></li><li><strong>write miss</strong>: the address being written to isn&rsquo;t cached. We have two
options:<ul><li><strong>write allocate</strong>: Also called fetch-on-write. As its alternate name
suggests, it involves fetching the associated block from memory into cache,
then treating it as a write hit (use either write-through or write-back
approaches). Advantages: exploits locality especially if paired with the
write-back approach. Disadvantage: every write-miss results in a block
transfer into the cache.</li><li><strong>no write allocate</strong>: on a write-miss, bypass the cache and write the data
unit directly to memory. The cached block has to be invalidated such that if
read soon after the write, it should result in a cache miss. It&rsquo;s
advantageous in situations where we&rsquo;re writing to an object that won&rsquo;t be
read any time soon hence we can avoid allocating cache lines that won&rsquo;t get
used (cache pollution).</li></ul></li></ul><p>Since write-back and write-allocate complement each other well with regards to
locality, they are often paired.</p><h2 class=heading id=exploiting-locality>Exploiting locality
<a href=#exploiting-locality>#</a></h2><p>Given that caches leverage locality, for optimal performance, we need to exploit
both temporal & spatial locality when writing our programs. Let&rsquo;s start with a
simple example, summing up all the values in a two dimensional array:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#902000>double</span> <span style=color:#06287e>get_sum</span>(<span style=color:#902000>double</span> a[M][N]){
</span></span><span style=display:flex><span>  <span style=color:#902000>double</span> res <span style=color:#666>=</span> <span style=color:#40a070>0.0</span>;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>double</span> i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> M; i<span style=color:#666>++</span>){
</span></span><span style=display:flex><span>      <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>double</span> j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>){
</span></span><span style=display:flex><span>          res <span style=color:#666>+=</span> a[i][j];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>return</span> res; 
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This seemingly simple snippet takes advantage of both temporal and spatial
locality. The 2-D array consists of <code>M</code> rows each containing <code>N</code> columns (i.e. N
doubles). In C, such arrays are laid out in memory in a
<a href=https://en.wikipedia.org/wiki/Row-_and_column-major_order>row-major order</a>.
Therefore, when summing up the elements in the above snippet, the inner-loop
iterates through the first row, then the second row and so forth [1]. The local
variable <code>res</code> should be cached in the CPU register by the compiler therefore
making iterative writes to it fast (temporal locality). Upon a cache miss,
chunks of the array are loaded into memory block by block. Thus, the next read
is a cache hit enabling the program to take advantage of spatial locality.</p><p>Now, suppose we interchange the i and j loops as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#902000>double</span> <span style=color:#06287e>get_sum</span>(<span style=color:#902000>double</span> a[M][N]){
</span></span><span style=display:flex><span>  <span style=color:#902000>double</span> res <span style=color:#666>=</span> <span style=color:#40a070>0.0</span>;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>double</span> j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>){
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>double</span> i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> M; i<span style=color:#666>++</span>){
</span></span><span style=display:flex><span>          res <span style=color:#666>+=</span> a[i][j];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>return</span> res; 
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The program traverses the 2-D array by going down the first column, then the
second and so on.</p><p><figure><div><img loading=lazy alt="row-major traversal vs column-major traversal" src=/p/matrix-mult-and-caching/images/traversals.svg></div></figure></p><p>It&rsquo;s still the same from a correctness perspective (though I&rsquo;m not quite sure
addition of doubles is commutative, but let&rsquo;s assume). And it still does the
same amount of work from a theoretic/complexity perspective (<code>O(MN)</code>),
particularly if we treat referencing random memory as a constant operation.
However, its &ldquo;actual&rdquo; performance is quite poor compared to the prior row-by-row
version. Given how 2-D arrays are laid out, it fails to take advantage of
spatial locality. In fact, if <code>N</code> is set to the size of the cache, each
inner-loop addition requires a fetch from a lower level and might even evict
previous rows that we&rsquo;ll need to access again when summing up the next column.
All these make the performance gap quite evident.</p><p>When carrying out a benchmark to demonstrate the performance gap between summing
row-by-row vs column-by-column, I get the following results:</p><p><figure><div><img loading=lazy alt="sum by row vs by col" src=/p/matrix-mult-and-caching/images/sum_by_row_vs_by_col.svg></div></figure></p><p>Summing row-by-row is 28.8% faster. In this case, each row has 512 doubles and
there are 1024 rows. As an aside, I ported the prior C code snippet to Rust and
used <a href=https://github.com/bheisler/criterion.rs>criterion</a> for the benchmarking
just to get a handle of how microbenchmarks can be carried out in Rust.</p><p>Regardless, in both cases, we see what&rsquo;s referred to as a <strong>stride pattern</strong>.
The row-by-row version exhibits a <em>stride-1</em> pattern - the best-case for spatial
locality. The column-by-column version exhibits a <em>stride-N</em> pattern, the larger
N is the more spatial locality decreases.</p><h2 class=heading id=key-takeaways-so-far>Key Takeaways so Far
<a href=#key-takeaways-so-far>#</a></h2><p>Let&rsquo;s pause a bit and take stock of the &lsquo;best practices&rsquo; highlighted so far (all
these are referenced from [1]):</p><ul><li>when iterating, take into account the stride and try to reduce it as much as
possible so as to exploit spatial locality. Stride-1/sequential access is
optimal given that contiguous data in memory is cached in blocks.</li><li>use/re-use a data-object as much as possible to maximize temporal locality</li><li>when optimizing for writes, for simplicity&rsquo;s sake, assume the caches are
write-back write-allocate. These already tends to be the case in modern
computer systems. For example in my laptop (purchased in 2020), the L1
d-cache,L2 and L3 caches are write-back and presumably write-allocate. In
linux, you can use the <a href=https://linux.die.net/man/8/dmidecode>dmidecode</a> tool
to check your cache info(<code>dmidecode -t cache</code>)</li><li>for nested loops, pay attention to the innermost loop since it&rsquo;s where the
bulk of the work is done.</li></ul><p>Now, back to matrix multiplication:</p><h2 class=heading id=ab-routines-for-matrix-multiplication>AB Routines for Matrix Multiplication
<a href=#ab-routines-for-matrix-multiplication>#</a></h2><p>Recall we started with the following straightforward matrix multiplication
routine:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#902000>int</span> i, j, k;
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> (i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> N; i<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#902000>double</span> r <span style=color:#666>=</span> <span style=color:#40a070>0.0</span>;
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (k <span style=color:#666>=</span> <span style=color:#40a070>0</span>; k <span style=color:#666>&lt;</span> N; k<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>            r <span style=color:#666>+=</span> A[i][k] <span style=color:#666>*</span> B[k][j];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        C[i][j] <span style=color:#666>=</span> r;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The outermost loop increments i, then the middle loop increments j, then the
innermost loop increments k. We shall refer to this as the <code>ijk</code> version.</p><p>Let&rsquo;s focus on the work being done in the innermost-loop:</p><pre tabindex=0><code>r += A[i][k] * B[k][j];
</code></pre><p>Given that k is incrementing in the innermost loop while i and j remain
constant, the traversal of A is row-wise with a stride of 1 while the traversal
of B is column-wise with a stride of n (n is assumed to be very large thus
negating any instance of spatial locality).</p><p><figure><div><img loading=lazy alt="AB traversals" src=/p/matrix-mult-and-caching/images/AB.svg></div></figure></p><p>Additionally, each iteration involves 2 loads (reads from A and B), and zero
stores. Suppose a cache line holds 64 bytes and a double is 8 bytes. With the
row-wise traversal of A, we&rsquo;ll get a cache miss when loading the 0th value in a
block and cache hits on the next 7 values. Therefore each iteration will involve
0.125 misses when referencing A. With the column-wise traversal of B, each
reference per iteration will result in a cache miss. We ignore the store
involving C since it&rsquo;s done outside and majority of the work/time spent will be
inside the loop. In total, there will be 1.125 misses per iteration.</p><p>Now that we have a bit of experience interchanging for-loops, we could switch
i-j to j-i as follows. This will be the <code>jik</code> version:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#902000>int</span> i, j, k;
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> (j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> N; i<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#902000>double</span> r <span style=color:#666>=</span> <span style=color:#40a070>0.0</span>;
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (k <span style=color:#666>=</span> <span style=color:#40a070>0</span>; k <span style=color:#666>&lt;</span> N; k<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>            r <span style=color:#666>+=</span> A[i][k] <span style=color:#666>*</span> B[k][j];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        C[i][j] <span style=color:#666>=</span> r;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>As we&rsquo;ll see in the benchmark results section, <code>ijk</code> and <code>jik</code> have essentially
the same performance due to having the same memory access pattern (1.125 misses
per innermost iteration) in the innermost for-loop. Thus, they both fall under
the same equivalence class of AB.</p><p>Can we do better? Yes, definitely:</p><h2 class=heading id=bc-routines-for-matrix-multiplication>BC Routines for Matrix Multiplication
<a href=#bc-routines-for-matrix-multiplication>#</a></h2><p>Let&rsquo;s start with the following diagram:</p><p><figure><div><img loading=lazy alt="Loop directions for i,j,k" src=/p/matrix-mult-and-caching/images/loop_directions.svg></div></figure></p><p>Observe that if we make the for-loop for j the innermost one, we&rsquo;ll get a
row-wise traversal of matrix B and C which will be of stride-1, thus maximizing
spatial locality.</p><p>Translating this into code, we get:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#902000>int</span> i, j, k;
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> (k <span style=color:#666>=</span> <span style=color:#40a070>0</span>; k <span style=color:#666>&lt;</span> N; k<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> N; i<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>          C[i][j] <span style=color:#666>+=</span> A[i][k] <span style=color:#666>*</span> B[k][j];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><code>A[i][k]</code> remains constant in the innermost loop therefore we can place it in a
temporary variable and the compiler will cache it in one of the CPU registers:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// kij
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span><span style=color:#902000>int</span> i, j, k;
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> (k <span style=color:#666>=</span> <span style=color:#40a070>0</span>; k <span style=color:#666>&lt;</span> N; k<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> N; i<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#902000>double</span> r <span style=color:#666>=</span> A[i][k];
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>          C[i][j] <span style=color:#666>+=</span> r <span style=color:#666>*</span> B[k][j];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>As is the case for the AB class, we can permute the k and i for-loops to get
<code>kij</code> and <code>ikj</code> versions:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// ikj
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span><span style=color:#902000>int</span> i, j, k;
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> (i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> N; i<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (k <span style=color:#666>=</span> <span style=color:#40a070>0</span>; k <span style=color:#666>&lt;</span> N; k<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#902000>double</span> r <span style=color:#666>=</span> A[i][k];
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>            C[i][j] <span style=color:#666>+=</span> r <span style=color:#666>*</span> B[k][j];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Both versions should have the same performance since they entail the same memory
access patterns:</p><p><figure><div><img loading=lazy alt="BC traversals" src=/p/matrix-mult-and-caching/images/BC.svg></div></figure></p><p>Let&rsquo;s analyze the work getting done per each iteration (of the innermost
for-loop):</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span>C[i][j] <span style=color:#666>+=</span> r <span style=color:#666>*</span> B[k][j];
</span></span></code></pre></div><p>Suppose the cache is write-back, write-allocate and a cache line holds 64 bytes.
Both B and C are being traversed row-wise with a stride of 1. We&rsquo;ve got two
loads: one for <code>B[k][j]</code>, and one for <code>C[i][j]</code>, which has to be loaded before
getting updated. This update does involve a subsequent store operation. The
store operation will not result in a write miss due to the preceding load. The
load involving A is ignored since it&rsquo;s carried out outside the loop. There will
be a cache miss every 0th load and the next 7 values will be straight cache
hits. Therefore there will be 0.25 (0.125 + 0.125) misses per iteration vs the
1.125 for AB routines.</p><p>Despite BC routines carrying out more memory accesses per iteration compared to
AB routines (2 loads and 1 store vs 2 loads only), they perform better courtesy
of less cache misses per iteration. The store doesn&rsquo;t end up hurting performance
as much since under a write-back system, the writes can go directly to the cache
and only during eviction does the memory get updated [4].</p><p>While we&rsquo;ve seen how we can get better performance, it&rsquo;s worth also seeing how
we can get way worse performance.</p><h2 class=heading id=ac-routines-for-matrix-multiplication>AC Routines for Matrix Multiplication
<a href=#ac-routines-for-matrix-multiplication>#</a></h2><p>By making the innermost for-loop be the one that increments <code>i</code>, we get the AC
routines: <code>jki</code> and <code>kji</code>. These traverse A and C column by column:</p><p><figure><div><img loading=lazy alt="AC traversals" src=/p/matrix-mult-and-caching/images/AC.svg></div></figure></p><p>Applying the same kind of analysis as for AB and BC routines, there will be 2
loads and a store per iteration and 2 cache misses per iteration due to the
stride of n.</p><p>For the sake of completion, here&rsquo;s the code sample for <code>jki</code> and <code>kji</code></p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// jki
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span><span style=color:#902000>int</span> i, j, k;
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> (j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (k <span style=color:#666>=</span> <span style=color:#40a070>0</span>; k <span style=color:#666>&lt;</span> N; k<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#902000>double</span> r <span style=color:#666>=</span> B[k][j];
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> N; i<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>            C[i][j] <span style=color:#666>+=</span> A[i][k] <span style=color:#666>*</span> r;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// kji
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span><span style=color:#902000>int</span> i, j, k;
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> (k <span style=color:#666>=</span> <span style=color:#40a070>0</span>; k <span style=color:#666>&lt;</span> N; k<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (j <span style=color:#666>=</span> <span style=color:#40a070>0</span>; j <span style=color:#666>&lt;</span> N; j<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#902000>double</span> r <span style=color:#666>=</span> B[k][j];
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> N; i<span style=color:#666>++</span>) {
</span></span><span style=display:flex><span>            C[i][j] <span style=color:#666>+=</span> A[i][k] <span style=color:#666>*</span> r;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 class=heading id=benchmark-results>Benchmark Results
<a href=#benchmark-results>#</a></h2><p>So far I&rsquo;ve been alluding to the great performance of BC routines without
showing the actual results. So without further ado, here&rsquo;s the graph comparing
all the routines:</p><p><figure><div><img loading=lazy alt="Line chart comparing total cycles for each matrix multiplication routine" src=/p/matrix-mult-and-caching/images/total_res.png width=1118px height=700px></div></figure></p><p>Routines within the same class have essentially the same performance, seeing as
their graphs are indistinguishable. The graphs&rsquo;s divided into three regions, in
the leftmost region, all data should fit entirely in L2; in the middle region,
all the data should fit in L3; in the rightmost region, the data is larger than
L3. However, I did make a mistake in the demarcation between L2 and L3, the size
of L2 in the graph is larger than it should be. Nonetheless, we see that once
data no longer fits in L3, the performance between the three classes starts
diverging with the AC class routines getting significantly slower compared to
the rest.</p><p>We&rsquo;ve also got the following:</p><p><figure><div><img loading=lazy alt="Line chart comparing cycles per inner-loop iteration for each matrix multiplication routine" src=/p/matrix-mult-and-caching/images/per_iter_res.png width=1118px height=700px></div></figure></p><p>In this, we measure how many cycles a single innermost loop takes, that is, one
instance of the multiplication and addition. BC(kij & ikj) remains consistent
throughout even as the number of elements increases. AB(ijk & jik) also remains
consistent though it spends roughly twice the number of cycles as BC. However,
with AC(jki and kji), once the data can no longer fit in L3, its performance
degrades heavily since every iteration entail two separate memory accesses with
the cache blocks not even getting re-used before they&rsquo;re evicted.</p><p>Regardless, as pointed out in [1], miss rate dominates performance which
demonstrates the need for factoring cache usage when designing and optimizing
algorithms.</p><h2 class=heading id=references>References
<a href=#references>#</a></h2><ol><li><a href=https://csapp.cs.cmu.edu/>Computer Systems: A Programmer&rsquo;s Perspective, 3rd Edition - O&rsquo;Hallaron D.,
Bryant R. - Chapter 6 - The Memory Hierarchy</a></li><li><a href=https://en.wikipedia.org/wiki/Matrix_multiplication>Matrix multiplication - wikipedia</a></li><li><a href=https://www.cs.cornell.edu/courses/cs3410/2013sp/lecture/18-caches3-w.pdf>Caches(Writing) - Weatherspoon H. - Lecture slides</a></li><li><a href=https://www.cs.cmu.edu/afs/cs/academic/class/15213-f15/www/>Lecture 12 - Cache Memories - Bryant R., Franchetti F., O&rsquo;Hallaron D. - CMU 2015 Fall: 15-213 Introduction to Computer
Systems</a></li></ol></div><div class=single-pagination><hr><div class=flex><div class=single-pagination-next><div class=single-pagination-container-next><div class=single-pagination-text>â</div><div class=single-pagination-text><a href=/p/rust-duckdb-py-udf/>Vectorized DuckDB UDFs with Rust and Python FFI</a></div></div></div><div class=single-pagination-prev><div class=single-pagination-container-prev><div class=single-pagination-text><a href=/p/cache-control-instrs/>x86 Cache Control Instructions</a></div><div class=single-pagination-text>â</div></div></div></div><hr></div><div class=back-to-top><a href=#top>back to top</a></div></div></main></div><footer><p>&mldr;</p></footer></body><script>function isAuto(){return document.body.classList.contains("auto")}function setTheme(){if(!isAuto())return;document.body.classList.remove("auto");let e="light";window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches&&(e="dark"),document.body.classList.add(e)}function invertBody(){document.body.classList.toggle("dark"),document.body.classList.toggle("light")}isAuto()&&window.matchMedia("(prefers-color-scheme: dark)").addListener(invertBody),setTheme()</script></html>