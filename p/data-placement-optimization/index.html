<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><link rel=icon type=image/ico href=https://bnmoch3.org/favicon.ico?><link rel=icon type=image/png sizes=16x16 href=https://bnmoch3.org/favicon-16x16.png?><link rel=icon type=image/png sizes=32x32 href=https://bnmoch3.org/favicon-32x32.png?><link rel=icon type=image/png sizes=192x192 href=https://bnmoch3.org/android-chrome-192x192.png?><link rel=apple-touch-icon sizes=180x180 href=https://bnmoch3.org/apple-touch-icon.png?><meta name=description content><title>Optimizing Data Placement for Distributed OLAP Systems | bnmoch3
</title><link rel=canonical href=https://bnmoch3.org/p/data-placement-optimization/><meta property="og:url" content="https://bnmoch3.org/p/data-placement-optimization/"><meta property="og:site_name" content="bnmoch3"><meta property="og:title" content="Optimizing Data Placement for Distributed OLAP Systems"><meta property="og:description" content="Using MIP solvers to model and optimize shard placement"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-16T00:00:00+00:00"><meta property="article:tag" content="Distributed Systems"><meta property="article:tag" content="Paper Review"><link rel=stylesheet href=/assets/combined.min.01980ad4202828eb32272e7b1654f79f3c0022c15b1c932668dff73dffaf7e88.css media=all></head><body class=light><div class=content><header><div class=header><div class=flex><p class=small><a href=/>/home</a></p><p class=small><a href=/about>/about</a></p><p class=small><a href=/posts>/posts</a></p><p class=small><a href=/notes>/notes</a></p><p class=small><a href=/tags>/tags</a></p></div></div></header><main class=main><div class=breadcrumbs><a href=/>Home</a>
<span class=breadcrumbs-separator>> </span><a href=/posts/>Posts</a>
<span class=breadcrumbs-separator>> </span><a class=breadcrumbs-current href=/p/data-placement-optimization/>Optimizing Data Placement for Distributed OLAP Systems</a></div><div><div class=single-intro-container><h1 class=single-title>Optimizing Data Placement for Distributed OLAP Systems</h1><p class=single-readtime><time datetime=2024-04-16T00:00:00+00:00>April 16, 2024</time>
&nbsp; Â· &nbsp;
17 min read</p></div><div class=single-content><p>In distributed OLAP database systems, tables and indices are partitioned into
shards and those shards are placed across a set of servers; queries are then
routed to the assigned shards&rsquo; servers during execution - basic stuff.</p><p>Of key is that the placement of the shards affects performance. As per the paper
&ldquo;<a href=https://dl.acm.org/doi/abs/10.14778/3574245.3574260>Parallelism-Optimizing Data Placement for Faster
Data-Parallel Computations</a>&rdquo;
authored by Nirvik Baruah and co: &ldquo;to minimize the tail latency of data-parallel
queries, it is critical to place data such that the data items accessed by each
individual query are spread across as many machines as possible so that each
query can leverage the computational resources of as many machines as possible&rdquo;.</p><p>A naive load-balancing scheme where we assume that any shard is equally likely
to be queried doesn&rsquo;t quite cut it, even though it&rsquo;s simple to implement. We
need to take into account the actual workload at hand and the characteristics of
that workload. Hence why the authors propose and analyze a placement scheme that
achieves the optimization of query parallelism. This post will go over the
paper&rsquo;s placement scheme with an implementation in python plus some caveats,
adjustments and corrections of my own.</p><p>The authors do name their solution as Parallelism-Optimizing Data Placement or
PODP in short.</p><h2 class=heading id=overview-of-podp>Overview of PODP
<a href=#overview-of-podp>#</a></h2><p>PODP formulates data placement as a mathematical optimization problem where
we&rsquo;ve got the objective of maximizing parallelism while minimizing data movement
every time the procedure is ran. All these are subject to a couple of
constraints such as &lsquo;all shards assigned to a server are within its memory
capacity&rsquo; and &rsquo;load is balanced across all the servers&rsquo;.</p><p>Maximizing query parallelism requires that we keep track of the most frequent
queries and the shards those queries access. It&rsquo;s then up to the solver to
figure out the optimal placement such that parallelism is maximized. After all,
the authors observer that given a workload: the &ldquo;worst-case latency is
proportional to the maximum number of co-located shards accessed by a query, as
under high loads those shards may be accessed sequentially instead of in
parallel&rdquo;. Colocation here refers to shards a query accesses that are hosted
within the same server. The colocation counts per server is a query&rsquo;s n-cluster
values. Across all the servers, the maximum n-cluster value is referred to as
the query&rsquo;s <em>clustering</em>. The goal is then to minimize the clustering across the
most frequent queries - or if feasible, all the queries. This in turn maximizes
query parallelism. Since the queries a system handles change over time, this
procedure has to be ran periodically.</p><p>The procedure above will result in lots of shard movement as the solver figures
out how to re-organize the queries&rsquo; shard sets. Hence the second step -
minimizing data movement while maintaining the optimal query parallelism values
we&rsquo;ve obtained from the first step. For this, we model the transfer costs of
placing a given shard at a specific server and minimize the overall cost across
all placements.</p><p>To reiterate, the optimization problem is divided into two steps:</p><ol><li>Maximize Query Parallelism</li><li>Minimize Data movement</li></ol><p>Now for the implementation:</p><h2 class=heading id=relevant-variables>Relevant Variables
<a href=#relevant-variables>#</a></h2><p>Let&rsquo;s consider a setup with 6 servers and 200 shards:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>num_servers <span style=color:#666>=</span> <span style=color:#40a070>6</span>
</span></span><span style=display:flex><span>num_shards <span style=color:#666>=</span> <span style=color:#40a070>200</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>shards <span style=color:#666>=</span> <span style=color:#007020>list</span>(<span style=color:#007020>range</span>(num_shards))
</span></span><span style=display:flex><span>servers <span style=color:#666>=</span> <span style=color:#007020>list</span>(<span style=color:#007020>range</span>(num_servers))
</span></span></code></pre></div><p>Each server has a max memory capacity and each shard takes up some amount of
memory. To keep things simple, every shard will have a size of 1 memory unit and
all servers will have the same amount of memory which when summed up will be
1.5x the size of the entire dataset:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># memory usage of each shard</span>
</span></span><span style=display:flex><span>shard_memory_usages <span style=color:#666>=</span> [<span style=color:#40a070>1</span> <span style=color:#007020;font-weight:700>for</span> _ <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)]
</span></span><span style=display:flex><span>server_max_memory <span style=color:#666>=</span> <span style=color:#007020>int</span>(
</span></span><span style=display:flex><span>    (num_shards <span style=color:#666>/</span> num_servers) <span style=color:#666>*</span> <span style=color:#40a070>1.5</span>
</span></span><span style=display:flex><span>)  <span style=color:#60a0b0;font-style:italic># maximum server memory</span>
</span></span></code></pre></div><p>Next, right before the data placement is ran, let&rsquo;s assume that shards are
placed across the servers randomly:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># entry[x][y] is 1 if server x has a copy of shard y and 0 otherwise</span>
</span></span><span style=display:flex><span>init_locations <span style=color:#666>=</span> [[<span style=color:#40a070>0</span> <span style=color:#007020;font-weight:700>for</span> _ <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)] <span style=color:#007020;font-weight:700>for</span> _ <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># assign shards to servers randomly</span>
</span></span><span style=display:flex><span>init_locations <span style=color:#666>=</span> gen_init_locations( <span style=color:#666>...</span> )
</span></span></code></pre></div><p>Now for the workload. After a given interval, let&rsquo;s say every 10 minutes, we
collect the statistics of the queries that have been ran. For optimizing data
placement, what we care about are the shards a query hits and the number of
times that given query was executed within the set interval, that is, its
frequency.</p><p>Queries are uniquely identified by the subset of shards they accessed rather
than by the content of the actual query. Therefore, if two or more different
queries hit the same shard subset, for all intents and purposes, they&rsquo;re
considered the same query and their frequencies are summed up. On the other
hand, if the same query hits different shards at different points in time (such
as in the case where a table is modified), then it&rsquo;s considered as different
queries. Again, all these is for the data placement optimization - other
downstream destinations for workload statistics will have their own
classifications and what-nots.</p><p>Given that some queries tend to occur more than others, I&rsquo;ve used a generation
scheme whereby 20% of the queries are assigned a higher frequency than the rest.
One might use a more complicated workload model to reflect real-life settings
but for now this will have to do for the sake of example:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>query_shard_set_size <span style=color:#666>=</span> <span style=color:#40a070>5</span>
</span></span><span style=display:flex><span>queries <span style=color:#666>=</span> []
</span></span><span style=display:flex><span>hot_queries <span style=color:#666>=</span> <span style=color:#007020>int</span>(<span style=color:#40a070>0.2</span> <span style=color:#666>*</span> num_shards)
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> q_id <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_queries):
</span></span><span style=display:flex><span>    frequency <span style=color:#666>=</span> <span style=color:#40a070>1</span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> q_id <span style=color:#666>&lt;</span> hot_queries:
</span></span><span style=display:flex><span>        frequency <span style=color:#666>=</span> <span style=color:#40a070>50</span>
</span></span><span style=display:flex><span>    shard_set <span style=color:#666>=</span> <span style=color:#007020>set</span>()
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> i <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(query_shard_set_size):
</span></span><span style=display:flex><span>        shard <span style=color:#666>=</span> (q_id <span style=color:#666>+</span> i) <span style=color:#666>%</span> num_shards
</span></span><span style=display:flex><span>        shard_set<span style=color:#666>.</span>add(shard)
</span></span><span style=display:flex><span>    queries<span style=color:#666>.</span>append(Query(q_id, shard_set, frequency))
</span></span></code></pre></div><p>Aside from the queries in the workload, we also need to keep track of the load
per shard. For this procedure, all shards will be initialized with a load of 1.
Once we&rsquo;ve got the query workloads, the loads of the shards per a given query
are updated accordingly:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>shard_loads <span style=color:#666>=</span> [<span style=color:#40a070>1</span> <span style=color:#007020;font-weight:700>for</span> _ <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)]
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> query <span style=color:#007020;font-weight:700>in</span> queries:
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> query<span style=color:#666>.</span>shard_set:
</span></span><span style=display:flex><span>        shard_loads[shard] <span style=color:#666>+=</span> query<span style=color:#666>.</span>frequency
</span></span></code></pre></div><p>The load per shard is key in balancing the total load across all the servers.
The average load per server before optimization is 1,833.33 units.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>average_load_per_server <span style=color:#666>=</span> <span style=color:#007020>sum</span>(shard_loads) <span style=color:#666>/</span> num_servers
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(average_load_per_server)
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># 1833.3333333333333</span>
</span></span></code></pre></div><p>However, if we calculate the actual load per server, we get the following:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>get_server_loads</span>(num_servers, shard_to_server_map, shard_loads):
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> [
</span></span><span style=display:flex><span>        <span style=color:#007020>sum</span>(
</span></span><span style=display:flex><span>            shard_loads[shard]
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>for</span> shard, present <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>enumerate</span>(shard_to_server_map[server])
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>if</span> present <span style=color:#666>==</span> <span style=color:#40a070>1</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>init_server_loads <span style=color:#666>=</span> get_server_loads(
</span></span><span style=display:flex><span>    num_servers, init_locations, shard_loads
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(<span style=color:#4070a0>&#34;init server loads: &#34;</span>, init_server_loads)
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># init server loads:  [1668, 1130, 1961, 2048, 2573, 1620]</span>
</span></span></code></pre></div><p>The load is distributed quite unevenly - the 1st server (zero-indexing) has a
load of 1130 while the 4th one has a load of 2573.</p><p>Let&rsquo;s also collect the query clustering:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># the set of shards in each server</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>get_server_shard_sets</span>(num_servers, shard_to_server_map):
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> [
</span></span><span style=display:flex><span>        <span style=color:#007020>set</span>(
</span></span><span style=display:flex><span>            shard
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>for</span> shard, present <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>enumerate</span>(shard_to_server_map[server])
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>if</span> present <span style=color:#666>==</span> <span style=color:#40a070>1</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>init_server_shard_sets <span style=color:#666>=</span> get_server_shard_sets(num_servers, init_locations)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># for each query, get its clustering</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>get_query_clustering</span>(num_servers, server_shard_sets, queries):
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> [
</span></span><span style=display:flex><span>        <span style=color:#007020>max</span>(
</span></span><span style=display:flex><span>            <span style=color:#007020>len</span>(query<span style=color:#666>.</span>shard_set<span style=color:#666>.</span>intersection(server_shard_sets[s]))
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>for</span> s <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> query <span style=color:#007020;font-weight:700>in</span> queries
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>init_query_clustering_vals <span style=color:#666>=</span> get_query_clustering(
</span></span><span style=display:flex><span>    num_servers, init_server_shard_sets, queries
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># for each query, get the weighted clustering, that is: if a query is more </span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># frequent, its weighted clustering should be higher and if it&#39;s less freqeuent</span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># its weighted clustering should be relatively lower even if the non-weighted</span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># clustering is higher</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>get_weighted_query_clustering_sum</span>(queries, query_clustering_vals):
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> <span style=color:#007020>sum</span>(
</span></span><span style=display:flex><span>        (q<span style=color:#666>.</span>frequency <span style=color:#666>*</span> cs) <span style=color:#007020;font-weight:700>for</span> q, cs <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>zip</span>(queries, query_clustering_vals)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>init_query_clustering_vals <span style=color:#666>=</span> get_query_clustering(
</span></span><span style=display:flex><span>    num_servers, init_server_shard_sets, queries
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Raw query clustering values might not be as useful since queries with low
frequency but high clustering values do not cause as much performance
degradation.</p><p>Next up, let&rsquo;s set up the MIP model for optimizing query parallelism:</p><h2 class=heading id=part-1-maximizing-query-parallelism>Part 1: Maximizing Query Parallelism
<a href=#part-1-maximizing-query-parallelism>#</a></h2><p>Ideally we&rsquo;d take a top-K sample of the workload sorted by the frequency after
filtering out queries that only hit a single shard (such a shard isn&rsquo;t colocated
with any other shard when it&rsquo;s being queried). For simplicity though I&rsquo;ll just
use all the queries.</p><p>Now for the model and decision variables. Recall that the goal is to minimize
tail latency via minimizing query clustering. Let&rsquo;s start by defining the
decision variables that will hold the the n-cluster values for the
<code>sample_queries</code> plus the objective. The minimum n-cluster value is 1 in which
case all the shards a frequent query accesses are spread uniformly across all
the servers available:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>pulp</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#666>=</span> pulp<span style=color:#666>.</span>LpProblem(<span style=color:#4070a0>&#34;p1&#34;</span>, pulp<span style=color:#666>.</span>LpMinimize)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>query_clustering_vars <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    pulp<span style=color:#666>.</span>LpVariable(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;c_</span><span style=color:#70a0d0>{</span>query<span style=color:#666>.</span>id<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>, lowBound<span style=color:#666>=</span><span style=color:#40a070>1</span>, cat<span style=color:#666>=</span><span style=color:#4070a0>&#34;Integer&#34;</span>) <span style=color:#007020;font-weight:700>for</span> query <span style=color:#007020;font-weight:700>in</span> queries
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># minimize the n-cluster values</span>
</span></span><span style=display:flex><span>p1_objective <span style=color:#666>=</span> pulp<span style=color:#666>.</span>lpSum(query_clustering_vars)
</span></span><span style=display:flex><span>model <span style=color:#666>+=</span> p1_objective
</span></span></code></pre></div><p>Without any constraints, the solver will simply set all the
<code>query_clustering_vars</code> to 1 and call it a day since that will minimize the
overall sum of the n-cluster variables. But don&rsquo;t worry, we&rsquo;ll add the
constraints quite soon. For now, there are a couple of aspects that can be
improved upon in the above objective definition.</p><p>Let&rsquo;s start by adding an upper bound to make the solver&rsquo;s work easier. The
upper-bound will be the largest number of shards that a single server can host.
Since all servers have the same memory capacity and all shards take up the same
amount of memory (1 unit), the upper bound is obtained as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>math</span>
</span></span><span style=display:flex><span>ub <span style=color:#666>=</span> math<span style=color:#666>.</span>ceil(num_shards <span style=color:#666>/</span> num_servers)
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(ub)
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># 20</span>
</span></span></code></pre></div><p>Next: we&rsquo;ve got the frequencies of the queries, we ought to weigh the clustering
variables by their respective frequencies so that the solver can focus more on
minimizing the clustering for the most frequent queries:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#666>+=</span> pulp<span style=color:#666>.</span>lpSum(
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        q<span style=color:#666>.</span>frequency <span style=color:#666>*</span> n_cluster
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> (q, n_cluster) <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>zip</span>(queries, query_clustering_vars)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p><code>lpSum</code> works fine but with such cases, it&rsquo;s more common to use the dot product
which you&rsquo;ll come across in lots of the MIP code out there.</p><p>Overall, we end up with the following:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>pulp</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>math</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#666>=</span> pulp<span style=color:#666>.</span>LpProblem(<span style=color:#4070a0>&#34;p1&#34;</span>, pulp<span style=color:#666>.</span>LpMinimize)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>query_clustering_vars <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    pulp<span style=color:#666>.</span>LpVariable(
</span></span><span style=display:flex><span>        <span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;c_</span><span style=color:#70a0d0>{</span>query<span style=color:#666>.</span>id<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>,
</span></span><span style=display:flex><span>        lowBound<span style=color:#666>=</span><span style=color:#40a070>1</span>,
</span></span><span style=display:flex><span>        upBound<span style=color:#666>=</span>math<span style=color:#666>.</span>ceil(num_shards <span style=color:#666>/</span> num_servers),
</span></span><span style=display:flex><span>        cat<span style=color:#666>=</span><span style=color:#4070a0>&#34;Integer&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> query <span style=color:#007020;font-weight:700>in</span> queries
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># minimize sum of query clustering weighted by query frequency</span>
</span></span><span style=display:flex><span>objective <span style=color:#666>=</span> pulp<span style=color:#666>.</span>lpDot(
</span></span><span style=display:flex><span>    query_clustering_vars, [query<span style=color:#666>.</span>frequency <span style=color:#007020;font-weight:700>for</span> query <span style=color:#007020;font-weight:700>in</span> queries]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#666>+=</span> objective
</span></span></code></pre></div><p>There are two addition classes of decision variables we need to define. The
first is the <code>assn_vars</code> which will determine where the shards are placed after
the solver runs. For example, given server 6 and shard 100, if
<code>assn_vars[6][100] == 1</code> then server 6 will be assigned shard 100. If server 6
already hosted shard 100 before the new assignments were carried out then
nothing happens; otherwise, it will have to either pull the shard from a node
that hosts it or have the coordinator or another node push the shard to it:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>assn_vars <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        pulp<span style=color:#666>.</span>LpVariable(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;a_</span><span style=color:#70a0d0>{</span>server<span style=color:#70a0d0>}</span><span style=color:#4070a0>_</span><span style=color:#70a0d0>{</span>shard<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>, <span style=color:#40a070>0</span>, <span style=color:#40a070>1</span>, cat<span style=color:#666>=</span><span style=color:#4070a0>&#34;Binary&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>We&rsquo;ve also got the <code>p_vars</code>. This is the proportion or probabilities with which
the coordinator should route a query pertaining a given shard to a specific
server. Suppose we&rsquo;ve got 5 shards. If the proportion row for server 2 is
<code>[0.5, 0.0, 0.0, 0.25, 0.0]</code>, then half the queries accessing the 0th shard
should be routed to server 2 and a quarter of the queries accessing the 3rd
shard should be routed to server 2. Server 2 should not expect to process
queries accessing the 1st, 2nd and 4th shards. On the same note, once the shards
are placed optimally, server 2 should only host the 0th and 3rd shard.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>p_vars <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        pulp<span style=color:#666>.</span>LpVariable(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;p_</span><span style=color:#70a0d0>{</span>server<span style=color:#70a0d0>}</span><span style=color:#4070a0>_</span><span style=color:#70a0d0>{</span>shard<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>, <span style=color:#40a070>0</span>, <span style=color:#40a070>1</span>, cat<span style=color:#666>=</span><span style=color:#4070a0>&#34;Continuous&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>With the decision variables in place, it&rsquo;s time to add the constraints.</p><h2 class=heading id=constraints>Constraints
<a href=#constraints>#</a></h2><p>The first constraint is as follows: for every query, no server should host more
shards than that query&rsquo;s clustering. The solver will figure out the optimal
clustering for each query so as to minimize the objective and as it&rsquo;s doing so,
the shard assignments should be consistent with the clustering values. When
formulating MIP, we need to make all our assumptions and constraints explicit:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> query, query_clusering_var <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>zip</span>(queries, query_clustering_vars):
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers):
</span></span><span style=display:flex><span>        model <span style=color:#666>+=</span> (
</span></span><span style=display:flex><span>            pulp<span style=color:#666>.</span>lpSum(
</span></span><span style=display:flex><span>                [assn_vars[server][shard] <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> query<span style=color:#666>.</span>shard_set]
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#666>&lt;=</span> query_clusering_var
</span></span><span style=display:flex><span>        )
</span></span></code></pre></div><p>To ensure that the values <code>p_vars</code> and <code>assn_vars</code> take up also remain
consistent, we need to add the following constraints:</p><ul><li>for a given shard, the proportions (which are in essence probabilities) with
which queries accessing it are routed to different servers should all add up
to 1</li><li>If the rate/probability at which a server should expect a query accessing a
given shard is greater than 0, then that server should be assigned that shard.
To phrase it differently, if <code>assn_vars[server][shard] == 0</code> then
<code>p_vars[server][shard] == 0</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># for a given shard, ensure if assn is 0 then p is 0</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers):
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards):
</span></span><span style=display:flex><span>        model <span style=color:#666>+=</span> (
</span></span><span style=display:flex><span>            p_vars[server][shard] <span style=color:#666>&lt;=</span> assn_vars[server][shard]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># for a given shard, the sum of all p should equal 1</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards):
</span></span><span style=display:flex><span>    model <span style=color:#666>+=</span> (
</span></span><span style=display:flex><span>        pulp<span style=color:#666>.</span>lpSum(p_vars[server][shard] <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers))
</span></span><span style=display:flex><span>        <span style=color:#666>==</span> <span style=color:#40a070>1</span>
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>Whichever assignment the solver figures out, each shard should be placed on at
least one server. In cases where the system must provide durability guarantees
via replication, this constraint could be modified to ensure the shards are
stored in say at least 3 servers if that&rsquo;s the replication factor:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># require each shard to appear on at least one server</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards):
</span></span><span style=display:flex><span>    model <span style=color:#666>+=</span> (
</span></span><span style=display:flex><span>        pulp<span style=color:#666>.</span>lpSum(
</span></span><span style=display:flex><span>            assn_vars[server][shard]
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#666>&gt;=</span> <span style=color:#40a070>1</span>
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>Lest we forget, we&rsquo;ve also got server memory constraints that must be adhered
to:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># server memory constraints</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers):
</span></span><span style=display:flex><span>    model <span style=color:#666>+=</span> (
</span></span><span style=display:flex><span>        pulp<span style=color:#666>.</span>lpDot(shard_memory_usages, assn_vars[server])
</span></span><span style=display:flex><span>        <span style=color:#666>&lt;=</span> server_max_memory
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>Lastly, we&rsquo;ve got the load balancing constraints. Setting the epsilon value is
tricky. We can make it smaller but that also risks making the problem
infeasible. On the other hand, making it too large and the load ends up
imbalanced across all the servers:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># load constraints</span>
</span></span><span style=display:flex><span>average_load_per_server <span style=color:#666>=</span> <span style=color:#007020>sum</span>(shard_loads) <span style=color:#666>/</span> num_servers
</span></span><span style=display:flex><span>epsilon <span style=color:#666>=</span> <span style=color:#40a070>0.05</span> <span style=color:#666>*</span> average_load_per_server
</span></span><span style=display:flex><span>server_load_bound <span style=color:#666>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;lower&#34;</span>: average_load_per_server <span style=color:#666>-</span> epsilon,
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;upper&#34;</span>: average_load_per_server <span style=color:#666>+</span> epsilon,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers):
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic># min load constraint</span>
</span></span><span style=display:flex><span>    model <span style=color:#666>+=</span> pulp<span style=color:#666>.</span>lpDot(shard_loads, p_vars[server]) <span style=color:#666>&gt;=</span> (
</span></span><span style=display:flex><span>        server_load_lowerbound[<span style=color:#4070a0>&#34;lower&#34;</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic># max load constraint</span>
</span></span><span style=display:flex><span>    model <span style=color:#666>+=</span> pulp<span style=color:#666>.</span>lpDot(shard_loads, p_vars[server]) <span style=color:#666>&lt;=</span> (
</span></span><span style=display:flex><span>        server_load_upperbound[<span style=color:#4070a0>&#34;upper&#34;</span>]
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>With some ad-hoc experimentation here and there, I&rsquo;ve figured that I can make
epsilon smaller by either increasing the server capacities (memory in this
case), or &lsquo;reducing&rsquo; the loads on the hottest shard sets relative to the rest -
or even combining both approaches.</p><p>In a production setting, increasing server capacities is straightforward. As for
the shard loads, we don&rsquo;t have that much control over the kind of queries
upstream clients will throw our way. What we can do instead is &lsquo;split&rsquo; hot shard
sets for the purposes of running the placement procedure. So for example if the
most frequently queried shard set is <code>{10,11,12,20,25}</code> with a frequency of 100,
we split it into two logical shard sets that all have the same shards but each
with a frequency of 50. With the current configurations though, none of this is
required.</p><p>Back to the problem at hand. With the objective and constraints in place, it&rsquo;s
time to run the solver:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>status <span style=color:#666>=</span> model<span style=color:#666>.</span>solve(pulp<span style=color:#666>.</span>PULP_CBC_CMD(msg<span style=color:#666>=</span><span style=color:#007020;font-weight:700>False</span>))
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>assert</span> (
</span></span><span style=display:flex><span>    status <span style=color:#666>==</span> pulp<span style=color:#666>.</span>constants<span style=color:#666>.</span>LpStatusOptimal
</span></span><span style=display:flex><span>), <span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;Unexpected non-optimal status </span><span style=color:#70a0d0>{</span>status<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimal_query_clustering_vals <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#007020>int</span>(v<span style=color:#666>.</span>varValue) <span style=color:#007020;font-weight:700>for</span> v <span style=color:#007020;font-weight:700>in</span> query_clustering_vars
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>Once done, <code>assn_vars</code> should hold the optimal shard placements and <code>p_vars</code>
should hold the optimal proportions with which the queries should be routed by
the executor. But we&rsquo;re not quite done yet&mldr;</p><h2 class=heading id=part-2-minimizing-data-movement>Part 2: Minimizing Data Movement
<a href=#part-2-minimizing-data-movement>#</a></h2><p>As is, placing shards around as per <code>assn_vars</code> will result in a lot of movement
and the system might end up spending more time shuffling data around for
non-query work rather than for executing actual queries.</p><p>With <code>init_locations</code>, let&rsquo;s derive a next-<code>assn_vars</code> that minimizes data
movement while maintaining the optimal clustering values for the top-K queries.</p><p>Let&rsquo;s start with the decision variables. We&rsquo;ll have the <code>p_vars</code> and <code>assn_vars</code>
once more but no <code>query_clustering_vars</code> since we already obtained the optimal
values from the first part:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>p_vars <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        pulp<span style=color:#666>.</span>LpVariable(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;p_</span><span style=color:#70a0d0>{</span>server<span style=color:#70a0d0>}</span><span style=color:#4070a0>_</span><span style=color:#70a0d0>{</span>shard<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>, <span style=color:#40a070>0</span>, <span style=color:#40a070>1</span>, cat<span style=color:#666>=</span><span style=color:#4070a0>&#34;Continuous&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>assn_vars <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        pulp<span style=color:#666>.</span>LpVariable(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;a_</span><span style=color:#70a0d0>{</span>server<span style=color:#70a0d0>}</span><span style=color:#4070a0>_</span><span style=color:#70a0d0>{</span>shard<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>, <span style=color:#40a070>0</span>, <span style=color:#40a070>1</span>, cat<span style=color:#666>=</span><span style=color:#4070a0>&#34;Binary&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>As for the objective function, the authors define it as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model2 <span style=color:#666>=</span> pulp<span style=color:#666>.</span>LpProblem(<span style=color:#4070a0>&#34;p2&#34;</span>, pulp<span style=color:#666>.</span>LpMinimize)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>p2_objective <span style=color:#666>=</span> pulp<span style=color:#666>.</span>lpSum(
</span></span><span style=display:flex><span>    <span style=color:#007020>list</span>(
</span></span><span style=display:flex><span>        init_locations[server][shard] <span style=color:#666>*</span> assn_vars[server][shard]
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model2 <span style=color:#666>+=</span> p2_objective
</span></span></code></pre></div><p>I might be wrong, but I reckon this definition achieves the exact opposite -
we&rsquo;re maximizing shard movement instead! From the paper:</p><blockquote><p>To model shard locations before assignment, we also define a matrix ð¡ where ð¡
ð ð is 0 if server ð currently hosts a replica of shard ð and 1 otherwise. The
total amount of shard movement is the sum of the element-wise product of ð¡ and
ð¥.</p></blockquote><p>Matrix T is <code>init_locations</code> and matrix X is <code>assn_vars</code>. With matrix T, an
element i,j is 1 if server i currently holds shard j. On the other hand, matrix
X holds the binary decision variables which indicate whether server i should
hold shard j. Given T and X, data movement occurs whenever <code>T[i][j] != X[i][j]</code>
(1 -> 0 and 0 -> 1). On the other hand, no data movement occurs if
<code>T[i][j] == X[i][j]</code> (1->1, 0->0). To minimize this element-wise product (as is
the case in the code snippet above), the solver has to reduce as many <code>1->1</code>s
i.e optimal placement of shards to where they were already residing (reduce data
&lsquo;rest&rsquo; or &lsquo;inertia&rsquo;). Additionally it&rsquo;ll flip as many 0s to 1s and 1s to 0&rsquo;s as
it can get away with (i.e. increase data movement). It&rsquo;s only in the case of
<code>0->0</code> where a server wasn&rsquo;t assigned a particular shard and still isn&rsquo;t that
the objective as defined aligns with minimizing data movement.</p><p>To actualize minimize data movement, we need to think of the costs of assigning
a particular shard at a given server. If the server already hosted that shard,
the cost is zero, otherwise, it&rsquo;s non-zero. For the sake of modeling, we can
derive the transfer costs from <code>init_locations</code> as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>transfer_costs <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    [<span style=color:#40a070>1</span> <span style=color:#666>-</span> init_locations[server][shard] <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)]
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>Rewriting the objective:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#666>=</span> pulp<span style=color:#666>.</span>LpProblem(<span style=color:#4070a0>&#34;p2&#34;</span>, pulp<span style=color:#666>.</span>LpMinimize)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>objective <span style=color:#666>=</span> pulp<span style=color:#666>.</span>lpSum(
</span></span><span style=display:flex><span>    <span style=color:#007020>list</span>(
</span></span><span style=display:flex><span>        transfer_costs[server][shard] <span style=color:#666>*</span> assn_vars[server][shard]
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>model <span style=color:#666>+=</span> objective
</span></span></code></pre></div><p>And now for the rest of the constraints.</p><p>We already obtained the optimal clustering values from part 1; let&rsquo;s extract it
from the decision variables then use it now as a constraint to ensure that
performance is maintained regardless of re-assignment:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> query, query_clustering_val <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>zip</span>(
</span></span><span style=display:flex><span>    queries, optimal_query_clustering_vals
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers):
</span></span><span style=display:flex><span>        model <span style=color:#666>+=</span> (
</span></span><span style=display:flex><span>            pulp<span style=color:#666>.</span>lpSum(
</span></span><span style=display:flex><span>                [assn_vars[server][shard] <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> query<span style=color:#666>.</span>shard_set]
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#666>&lt;=</span> query_clustering_val
</span></span><span style=display:flex><span>        )
</span></span></code></pre></div><p>The rest of the constraints are the same from part 1:</p><ul><li>server load constraints for load balancing</li><li>ensure <code>p_vars</code> and <code>assn_vars</code> are consistent</li><li>ensure server memory constraints are adhered to</li><li>require each shard to appear on at least one server</li></ul><p>Once the solver runs and we&rsquo;ve successfully obtained an optimal solution, it&rsquo;s
time to extract the assignments:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>status <span style=color:#666>=</span> model<span style=color:#666>.</span>solve(pulp<span style=color:#666>.</span>PULP_CBC_CMD(msg<span style=color:#666>=</span><span style=color:#007020;font-weight:700>False</span>))
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>assert</span> (
</span></span><span style=display:flex><span>    status <span style=color:#666>==</span> pulp<span style=color:#666>.</span>constants<span style=color:#666>.</span>LpStatusOptimal
</span></span><span style=display:flex><span>), <span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;Unexpected non-optimal status </span><span style=color:#70a0d0>{</span>status<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>next_locations <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    [<span style=color:#007020>int</span>(assn_vars[server][shard]<span style=color:#666>.</span>varValue) <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)]
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>next_query_routing_map <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    [p_vars[server][shard]<span style=color:#666>.</span>varValue <span style=color:#007020;font-weight:700>for</span> shard <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_shards)]
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> server <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_servers)
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p><code>next_locations</code> is used to place shards into the requisite servers and once
done, <code>next_query_routing_map</code> is used by the leader node to route queries to
the servers based on the shards each query is accessing.</p><p>After optimization, we get two important outcomes as per the given workload:</p><ul><li>the average load per server remains around the same ballpark: 1,833 initial
value vs. 1,875</li><li>the load is distributed more evenly:<ul><li>initial: <code>[1668, 1130, 1961, 2048, 2573, 1620]</code></li><li>next: <code>[1919, 1913, 1907, 1827, 1870, 1815]</code></li></ul></li><li>the weighted sum of query clustering is reduced:<ul><li>initial: 4208</li><li>next: 2161</li></ul></li></ul><p>Also of note, minus the second step of minimizing data movement, we&rsquo;d have a
transfer cost of 346 but with the step, we get a transfer cost 169 while still
maintaining the optimal query clustering and load balancing.</p><p>This approach should work for small input sizes (number of shards, servers and
queries) but will be infeasible for larger inputs. The authors do have a section
in the paper where they apply partitioning of the inputs and solving data
placement within those partitions so as to scale PODP. They go a bit deeper into
this partitioning approach in a separate paper:
<a href=https://people.eecs.berkeley.edu/~matei/papers/2021/sosp_pop.pdf>Solving Large-Scale Granular Resource Allocation
Problems Efficiently with POP</a>.
On a different note, PODP can be modified to work in settings where the cluster
size is dynamic. That is if the average load per server increases beyond a
certain threshold, we add more nodes to the cluster; if it goes below a set
threshold, we reduce the number of nodes - an exercise left to the reader :)</p><script src=https://giscus.app/client.js data-repo=bnmoch3/blog data-repo-id=R_kgDOIU86DQ data-category data-category-id=DIC_kwDOIU86Dc4Clvgl data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=gruvbox_light data-lang=en data-loading=lazy crossorigin=anonymous async></script></div><div class=single-pagination><hr><div class=flex><div class=single-pagination-next><div class=single-pagination-container-next><div class=single-pagination-text>â</div><div class=single-pagination-text><a href=/p/dbms-hybrid-locking/>Hybrid Locking & Synchronization</a></div></div></div><div class=single-pagination-prev><div class=single-pagination-container-prev><div class=single-pagination-text><a href=/p/duckdb-jit-udfs-numba/>DuckDB JIT Compiled UDFs with Numba</a></div><div class=single-pagination-text>â</div></div></div></div><hr></div><div class=back-to-top><a href=#top>back to top</a></div></div></main></div><footer><p>&mldr;</p></footer><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script></body><script>function isAuto(){return document.body.classList.contains("auto")}function setTheme(){if(!isAuto())return;document.body.classList.remove("auto");let e="light";window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches&&(e="dark"),document.body.classList.add(e)}function invertBody(){document.body.classList.toggle("dark"),document.body.classList.toggle("light")}isAuto()&&window.matchMedia("(prefers-color-scheme: dark)").addListener(invertBody),setTheme()</script></html>