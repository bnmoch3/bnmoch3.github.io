<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><link rel=icon type=image/ico href=https://bnmoch3.org/favicon.ico?><link rel=icon type=image/png sizes=16x16 href=https://bnmoch3.org/favicon-16x16.png?><link rel=icon type=image/png sizes=32x32 href=https://bnmoch3.org/favicon-32x32.png?><link rel=icon type=image/png sizes=192x192 href=https://bnmoch3.org/android-chrome-192x192.png?><link rel=apple-touch-icon sizes=180x180 href=https://bnmoch3.org/apple-touch-icon.png?><meta name=description content><title>Leanstore: High Performance Low-Overhead Buffer Pool | bnmoch3
</title><link rel=canonical href=https://bnmoch3.org/p/leanstore/><meta property="og:url" content="https://bnmoch3.org/p/leanstore/"><meta property="og:site_name" content="bnmoch3"><meta property="og:title" content="Leanstore: High Performance Low-Overhead Buffer Pool"><meta property="og:description" content="A dash of pointer swizzling, a sprinkle of optimistic locking and a touch of lean eviction, that’s the secret to a high performance buffer pool!"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-11T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-11T00:00:00+00:00"><meta property="article:tag" content="Database Internals"><meta property="article:tag" content="Paper Review"><link rel=stylesheet href=/assets/combined.min.01980ad4202828eb32272e7b1654f79f3c0022c15b1c932668dff73dffaf7e88.css media=all></head><body class=light><div class=content><header><div class=header><div class=flex><p class=small><a href=/>/home</a></p><p class=small><a href=/about>/about</a></p><p class=small><a href=/posts>/posts</a></p><p class=small><a href=/notes>/notes</a></p><p class=small><a href=/tags>/tags</a></p></div></div></header><main class=main><div class=breadcrumbs><a href=/>Home</a>
<span class=breadcrumbs-separator>> </span><a href=/posts/>Posts</a>
<span class=breadcrumbs-separator>> </span><a class=breadcrumbs-current href=/p/leanstore/>Leanstore: High Performance Low-Overhead Buffer Pool</a></div><div><div class=single-intro-container><h1 class=single-title>Leanstore: High Performance Low-Overhead Buffer Pool</h1><p class=single-readtime><time datetime=2024-06-11T00:00:00+00:00>June 11, 2024</time>
&nbsp; · &nbsp;
16 min read</p></div><div class=single-content><h2 class=heading id=overview>Overview
<a href=#overview>#</a></h2><p>Leanstore is what you get when you break down the core functionalities of a
buffer pool and for each functionality, take the lowest-overheard/highest
performance approach possible while keeping modern hardware in mind.</p><p>Let&rsquo;s start of with the numbers and charts showcasing Leanstore&rsquo;s performance:</p><p>Leanstore has <a href=/p/larger-than-mem-intro>two boxes to tick</a>: (1) does it offer
great performance in the case where the working set fits entirely in-memory, and
(2) does it offer decent larger-than-memory performance as the working set&rsquo;s
size increases.</p><p>In the first case where the working dataset fits entirely in-memory, a b-tree
implemented on top of Leanstore has almost the same performance as an in-memory
b-tree; both of course outperform traditional disk-based b-trees (graph sourced
from [1]):</p><p><figure><div><img loading=lazy alt="Figure 1" src=/p/leanstore/images/single_threaded_tpc_100_warehouses.png width=1369px height=492px></div></figure></p><p>And with multiple threads (graph sourced from [1]):</p><p><figure><div><img loading=lazy alt="Figure 8" src=/p/leanstore/images/mt_tpc_100_warehouses.png width=1316px height=839px></div></figure></p><p>Leanstore is designed with scalability in mind hence why it scales quite well as
number of threads increases. If a page already is in-memory, a thread doesn&rsquo;t
have to acquire any lock such as for the hash table to translate the page ID or
for the updating the cache eviction state.</p><p>For the second case (larger-than-memory performance), once the working dataset
exceeds main-memory, Leanstore offers a smooth off-ramp compared to other
systems and even the OS itself (swapping):</p><p><figure><div><img loading=lazy alt="Figure 9" src=/p/leanstore/images/larger_than_memory.png width=1319px height=902px></div></figure></p><p>As for the core functionalities required out of any buffer pool in a database,
the authors list the following:</p><ol><li>Page translation from disk-resident page identifier to in-memory location</li><li>Cache management: determining what pages should be kept in memory and which
ones should be evicted once more memory is required</li><li>I/O operations management: loading pages from disk, flushing dirty pages as
needed</li></ol><p>These are handled within 3 separate data structures in Leanstore:</p><ol><li>Buffer pool - handles page translation</li><li>Cooling stage - keeps a pool of pages ready for eviction should space be
needed</li><li>I/O Component: manages in-flight I/O operations</li></ol><p><figure><div><img loading=lazy alt="Figure showing states of a page in Leanstore" src=/p/leanstore/images/figure_1_leanstore_datastructures.png width=882px height=961px></div></figure></p><p>Worth mentioning, since multiple threads will be accessing pages, the buffer
pool also needs to provide some means for synchronizing threads.</p><p>Let&rsquo;s go over all the functionalities plus their associated data structures:</p><h2 class=heading id=page-translation-pointer-swizzling--io-management>Page Translation (Pointer Swizzling & I/O Management)
<a href=#page-translation-pointer-swizzling--io-management>#</a></h2><p>Traditional buffer pools use a hash map to translate page IDs to buffer frames
which in turn hold the in-memory address of the page (if it&rsquo;s already cached).
This adds significant overhead for page hits and is a scalability bottleneck
since threads have to acquire a lock before accessing the table.</p><p>Leanstore instead opts for <em>pointer swizzling</em>. I&rsquo;ve covered pointer swizzling
before as pertains to buffer pools (the
<a href=/notes/2024/pointer-swizzling>Goetz Graefe et al approach</a>); Leanstore&rsquo;s
approach is almost the same except that it uses virtual addresses directly for
pages that are already residing in memory.</p><p>A page can only have one other page holding a reference to it - or rather, every
page has one parent/one owner (a parent can of course have multiple children
such as btree root and inner nodes, or the array of containers in roaring
bitmaps). This restriction limits the kind of data structures that can be
implemented atop Leanstore. Nonetheless, it is key for simplifying correctness
and for high performance (as we shall see in Leanstore&rsquo;s replacement
strategies). To see why a child should only have one parent, suppose Leanstore
allowed for a child to have several parents; if a worker thread wants to swizzle
the child, it has to &lsquo;consult&rsquo; and coordinate with all the other parents so that
they are all aware that the child is swizzled. Same case with unswizzling. Such
a scenario negates all the performance gains from swizzling, one might as well
go back to using a hash map and a traditional buffer pool to centralize all such
state.</p><p>Back to the actual Leanstore implementation: let&rsquo;s start with the case where a
parent holds a reference to a child page and that page is on secondary storage
(has to be fetched/loaded). Here are the steps that a worker thread takes:</p><ol><li>Check that the reference indicates the page is on disk. References take up 8
bytes. One bit is used to indicate whether a page is on disk or in memory.
Once in-memory, only 48-bits (6 bytes) out of the 8 byte virtual address in
x86 64 bit machine are used for addressing, so we&rsquo;ve got a bit more leg room
to tag metadata compared to the on-disk page ID.</li><li>If the page is on disk, access the I/O component and initialize the I/O as
follows:<ul><li>Acquire the (global) lock of the hash table within I/O Component. This hash
tables maps on-disk page IDs to I/O frames. The authors argue that this
lock will not impede scalability & performance since either way, I/O
operations are slower than the preceding lock acquisition [1].</li><li>Create an I/O frame and insert it into the hash table. This consists of an
operating system mutex and a pointer to the in-memory location where the
page will be loaded to. A key advantage of OS mutexes is that they make
threads waiting in a low overhead resource-efficient manner (compared to
spin loops).</li><li>Acquire the mutex within the I/O frame before proceeding</li><li>Release the hash table&rsquo;s global lock</li><li>Issue the read system call. Note that the hash table&rsquo;s lock is released to
enable concurrent I/O operations. Once the read is complete, release the
I/O frame&rsquo;s mutex</li></ul></li><li>Suppose a different thread had already issued the I/O request for the given
page: the current thread will find that there&rsquo;s already an entry for the page
ID in the hash table then block on its mutex until the load is done.</li><li>Once the page is loaded, the reference to the page is swizzled (on-disk page
identifier replaced with in-memory address) and access proceeds as normal</li></ol><p>Suppose a page was already swizzled; the worker thread has only one step before
accessing the page: an if statement to check the bit indicator, that&rsquo;s it!.</p><h3 class=heading id=scaling-io>Scaling I/O
<a href=#scaling-io>#</a></h3><p>The latest iteration of Leanstore gets rid of the single lock over the hash
table for managing I/O operations [6]. Turns out the authors&rsquo; initial argument
(that a single lock over will not become a bottleneck) got invalidated with SSDs
getting faster and their I/O bandwidth increasing:</p><blockquote><p>With one SSD, the single I/O stage with the global mutex in the original
LeanStore design from 2018 was enough to saturate the SSD bandwidth. With
today’s PCIe 4.0, we can attach an array of, e.g., ten NVMe SSDs&mldr;, where
each drive can deliver up to 7000 MB/s read bandwidth and 3800 MB/s write
bandwidth. In such a system, we need high concurrency to issue a large number
of parallel I/O requests to fully utilize all SSDs. The global mutex that
protects the I/O hash table would quickly become a scalability bottleneck and
prevent us from reaching exploiting the maximum bandwidth that modern flash
drives can deliver. [6]</p></blockquote><p>Currently, Leanstore uses a partitioned I/O stage with the page ID determining
which partition will be used to handle and synchronize the operation [6]. Each
partition is protected by a lock and allows concurrent threads requesting the
same page to synchronize their requests and also to &lsquo;park&rsquo; as they wait along.</p><h2 class=heading id=replacement-strategy-1-leanevict>Replacement Strategy 1: LeanEvict
<a href=#replacement-strategy-1-leanevict>#</a></h2><p>Memory is a finite resources and eventually, a buffer pool must decide what to
evict and what to keep around. Hence all the caching algorithms designed for
eviction (LRU, LFU, Clock, Clock-Pro and so on).</p><p>Almost all common caching algorithms require some caching state to be updated
during a page hit. This range from the simple low-overhead approach in
Clock/Second Chance where a bit is set, to the higher-overhead approach in LRU
that requires the accessed entry to be moved to the head of a linked list. And
as we&rsquo;ve seen in previous larger-than-memory approaches, developers have to come
up with different ways for mitigating this overhead, from only using a sample of
the accesses (as is the case in <a href=/notes/2024/anti-caching>anticaching</a>), to
moving the analysis entirely offline to a different thread or process and only
logging the tuple accesses (as is the case in
<a href=/notes/2024/project-siberia-hot-cold-id>Project Siberia</a> and the
<a href=/notes/2024/efficient-os-paging-hot-cold-db>Stoica and Ailamaki approach</a>).</p><p>Leanstore takes a somewhat different philosophy [1]:</p><blockquote><p>Our replacement strategy reflects a change of perspective: Instead of tracking
frequently accessed pages in order to avoid evicting them, our replacement
strategy identifies <em>infrequently</em>-accessed pages. We argue that with the
large buffer pool sizes that are common today, this is much more efficient as
it avoids any additional work when accessing a hot page</p></blockquote><p>To reiterate, during a <em>page hit</em>, there&rsquo;s absolutely no cache state to be
updated. In fact, the only overhead is checking if the reference is swizzled
(single bit).</p><p>Instead, Leanstore maintains a steady pool of <em>cooling</em> pages that can be
evicted should space be needed. These are pages that are speculatively
unswizzled (must be referenced through their page IDs) but still kept around in
memory. It works this way:</p><ol><li>A random page is picked (as a candidate for eviction) and added to the
cooling stage.</li><li>If a page has one or more swizzled children, it&rsquo;s never unswizzled. Instead,
it randomly offers one of its swizzled children for eviction (so much for
good parenting). This is key for correctness since swizzled references are
only valid for the current program execution and should not be persisted.
Furthermore, from a caching perspective, given that only tree-like data
structures can be implemented atop Leanstore and worker threads have to
traverse from root to target nodes, parent pages will always be at least as
hot as their children - or rather, children cannot be hotter than their
parents therefore are &lsquo;better&rsquo; candidates for eviction.</li><li>Once a page is unswizzled, its parent is &rsquo;notified&rsquo;. Note that a page&rsquo;s
buffer frame also holds a pointer to its parent. Keeping around the parent
pointer is not dangerous since children are always unswizzled before their
parents.</li><li>Accessing a page that is in the cooling stage causes it to be re-swizzled:
the cooling stage can be thought of as a probational stage whereby if the
random page picked is in fact hot and should not be evicted, then it is given
a period to &lsquo;prove its hotness&rsquo; [1].</li><li>The cooling stage consists of a hash map and a FIFO queue. New entrants are
added at the head, the oldest cooling page at the tail is evicted should
space be needed</li><li>There&rsquo;s a background thread that iterates through the FIFO queue entries and
writes out dirty pages such that upon eviction, a worker thread is not likely
to incur the cost of writing a dirty page to disk.</li></ol><p>This image (from the paper [1]) shows all the states a page can be in:</p><p><figure><div><img loading=lazy alt="Figure showing states of a page in Leanstore" src=/p/leanstore/images/figure_3_possible_states_of_a_page.png width=772px height=531px></div></figure></p><p>At any point in time, a fraction of the cached pages are kept in the cooling
stage. The authors recommend a value between 5% - 20% which they arrive at
experimentally, with 10% being a decent starting point. If the working set is
close to the buffer pool size, a higher percentage means the system will spend a
lot of time swizzling and unswizzling hot pages [1].</p><p>Also, like any replacement algorithm worth its salt, LeanEvict implements some
form of <em>scan resistance</em>: during scans: a worker thread can pre-emptively
unswizzle a loaded page and add it to the cooling stage.</p><p>For the sake of comparison, the authors collect traces and compare how LeanEvict
fares against other cache eviction algorithms. In one particular trace, an
optimum caching strategy would give a 96.3% hit rate; LeanEvict has a 92.8% hit
rate while LRU is at 93.1%; and 2Q (everyone&rsquo;s favourite patented caching
algorithm) is at 93.8%.</p><h2 class=heading id=replacement-strategy-2-swizzling-based-second-chance-implementation>Replacement Strategy 2: Swizzling-Based Second Chance Implementation
<a href=#replacement-strategy-2-swizzling-based-second-chance-implementation>#</a></h2><p>Right when I though Leanstore&rsquo;s replacement approach couldn&rsquo;t get any better,
the authors go ahead to switch to something simpler that still does the job
quite fine - turns out the Cooling Stage was adding unnecessary complexity [6].
Currently, Leanstore uses Second Chance replacement. A swizzled page is assumed
to be, or rather starts of as hot, so no need to set the hit bit upon a cache
hit as is the case in the textbook version of Second Chance. Hence, accessing a
hot page still incurs the same cost as in the initial version.</p><p>As for selecting candidates for eviction, a random set of 64 buffer frames is
sampled. Within that set, Leanstore carries out the following actions depending
on the page&rsquo;s states:</p><ul><li>Hot page: change page&rsquo;s status to cool, &lsquo;inform&rsquo; the page&rsquo;s parent that its
child is cool. Unlike, the previous implementation whereby the parent swaps
the virtual address with a page ID, in the current implementation, the parent
only has to tag the virtual address as cool (set the MSB to 1)</li><li>Cool page: if page is dirty write back to disk, or evict immediately otherwise
[6]</li></ul><p>With this approach in place, Leanstore no longer needs the cooling stage&rsquo;s hash
table and FIFO queue. Also, instead of burdening worker threads with the task of
cooling and evicting pages, the current iteration has background Page Provider
threads that carry out the cooling and eviction. I&rsquo;m curious though: if
Leanstore still allows for cool pages that are still in-memory to transition
back to hot upon a hit and if so, how foreground worker threads that &lsquo;reheat&rsquo; a
page coordinate with the background page provider threads that might evict that
same page. This is probably one of those details that you have to read in the
actual code since there&rsquo;s only so much detail that can be included in the paper.</p><h2 class=heading id=synchronization>Synchronization
<a href=#synchronization>#</a></h2><p>With regards to page specific synchronization, Leanstore uses optimistic
latches: each page has a counter: writers increment the counter before beginning
modifications and readers &ldquo;can proceed without acquiring any latches, but
validate their reads using the version counters instead (similar to optimistic
concurrency control)&rdquo; [1].</p><p>What of the case where one thread is reading a page and another thread wants to
evict or delete the page (optimistic locks don&rsquo;t block such threads). To begin
with, Leanstore does not allow for unswizzled evicted pages to be reused
immediately. Instead it uses <em>epoch-based reclamation</em> to determine when it&rsquo;s
safe to reuse a page. I&rsquo;m not sure of the degree to which Leanstore&rsquo;s EBR
implementation differs from the textbook/canonical approach, so my following
overview of Leanstore&rsquo;s EBR might be a bit hand-wavey or worse, incorrect
(please check the original paper and code for correctness). With EBR, there&rsquo;s a
global epoch that&rsquo;s incremented every once in a while and each thread also has a
local epoch and a flag for indicating that it&rsquo;s in the critical section. In
Leanstore&rsquo;s case, rather than use a separate flag, the local epoch is set to a
sentinel value (infinity). When unswizzling a page (adding it to the cooling
stage), the thread doing so also attaches the value of the global epoch at the
time of unswizzling. As time goes by, the unswizzled page moves towards the end
of the FIFO queue at which point it can be evicted and the resources its holding
up (memory pages) be re-used by some other data. However, before eviction, we
still are not quite sure if there&rsquo;s a concurrent reader accessing it. Therefore,
we compare the unswizzled page&rsquo;s epoch with all the local epochs of threads; if
it&rsquo;s strictly less than all the local epochs, then we&rsquo;re 100% sure that no
thread is concurrently accessing the page. Note that if some threads aren&rsquo;t
doing any page accesses currently, then their local epochs are set to infinity
which means they won&rsquo;t impede resource reclamation.</p><p>Actually do ignore the last paragraph on Leanstore&rsquo;s EBR: the current Leanstore
implementation does something better than EBR which is nothing at all [2]!. Upon
startup, Leanstore pre-allocates the entire memory region for the buffer pool;
This memory is not released back to the OS until the process exits [1, 2]. Each
buffer frame already contains a version counter that&rsquo;s used for optimistic
locking. When evicting a page, the associated lock is held exclusively and the
version counter is incremented; it&rsquo;s not reset across evictions. If one thread
is reading a page and another thread is concurrently evicting it, the latter
thread can immediately re-use the page for new data after incrementing the
version counter; the reader will compare against the counter&rsquo;s new value and
retry the operation.</p><p>The initial implementation for Leanstore relied on spinlocks for exclusive
locks. What much can be said about spinlocks - easy to implement, easy to assume
they provide high performance but they are a
<a href=https://matklad.github.io/2020/01/02/spinlocks-considered-harmful.html>bad idea</a>.
Given that many of the researchers developing Leanstore also published papers on
<a href=/p/dbms-hybrid-locking>hybrid locking</a> and lightweight synchronization
primitives, it was only a matter of time before they adopted their techniques
within Leanstore [5,6]. In the current Leanstore implementation, pages support
optimistic locking (for short reads), shared pessimistic mode (blocks out
concurrent writers so no need to restart, for longer page reads) and exclusive
mode (mostly for writers, allows for threads waiting in line to be efficiently
&lsquo;parked&rsquo; rather than loop endlessly) [5, 6].</p><p>Unsurprisingly, Leanstore still retains its excellent performance that scales as
the number of threads increases [6]. WiredTiger (RU) comes quite close in
read-only workloads but that&rsquo;s only without any concurrency control, shared
memory reclamation (hazard pointers) and lock-free techniques whatsoever - the
Leanstore numbers are with its synchronization primitives still &rsquo;turned&rsquo; on
(both charts sourced from [6]):</p><p><figure><div><img loading=lazy alt="Figure 6,7" src=/p/leanstore/images/in_mem_scalability.png width=1823px height=843px></div></figure></p><h2 class=heading id=umbra-variable-size-pages>Umbra: Variable-size Pages
<a href=#umbra-variable-size-pages>#</a></h2><p>Umbra&rsquo;s [4] buffer pool builds upon Leanstore by offering variable-size pages.
The authors note that [4]:</p><blockquote><p>&mldr; a buffer manager with variable-size pages, which allows for storing large
objects natively and consecutively if needed. Such a design leads to a more
complex buffer manager, but it greatly simplifies the rest of the system. If
we can rely upon the fact that a dictionary is stored consecutively in memory,
decompression is just as simple and fast as in an in-memory system. In
contrast, a system with fixed-size pages either needs to re-assemble (and thus
copy) the dictionary in memory, or has to use a complex and expensive lookup
logic.</p></blockquote><p>Pages in umbra are partitioned into size classes - the smallest size class is 64
KiB with sizes per class doubling all the way possibly up to the buffer pool&rsquo;s
entire size. On startup, different private anonymous mappings (not backed by any
files) are reserved for each size class - these don&rsquo;t consume any physical
memory yet. An unswizzled pointer (page ID) contains both the tag (indicating
it&rsquo;s disk-resident) and the size class. Upon reading from disk (via the <code>pread</code>)
system class, it&rsquo;s loaded into the mmap region for its size class and the
pointer is swizzled. Once swizzled, its pointer does not need to contain the
size class it belongs to since that can be derived based on the starting virtual
address. When evicting a page, the <code>madvise</code> system call with the
<code>MADV_DONTNEED</code> flag is used so that the underlying physical pages of
main-memory can be reused.</p><h2 class=heading id=references>References
<a href=#references>#</a></h2><ol><li><a href=https://db.in.tum.de/~leis/papers/leanstore.pdf>LeanStore: In-Memory Data Management Beyond Main Memory - Viktor Leis,
Michael Haubenschild, Alfons Kemper, Thomas Neumann</a></li><li><a href=https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/reclaim.pdf>Lock-Free Buffer Managers Do Not Require Delayed Memory Reclamation - Michael Haubenschild, Viktor Leis</a></li><li><a href="https://www.youtube.com/watch?v=o467OKy7Q0g">Leanstore - Viktor Leis - CMU DB Talks</a></li><li><a href=https://www.cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf>Umbra: A Disk-Based System with In-Memory Performance - Thomas Neumann,
Michael Freitag</a></li><li><a href=https://db.in.tum.de/~boettcher/locking.pdf>Scalable and Robust Latches for Database Systems - Jan Böttcher, Viktor Leis, Jana Giceva, Thomas Neumann, Alfons Kemper</a></li><li><a href=https://dl.gi.de/server/api/core/bitstreams/edd344ab-d765-4454-9dbe-fcfa25c8059c/content>The Evolution of LeanStore - Adnan Alhomssi, Michael Haubenschild, Viktor Leis</a></li></ol></div><div class=single-pagination><hr><div class=flex><div class=single-pagination-next><div class=single-pagination-container-next><div class=single-pagination-text>←</div><div class=single-pagination-text><a href=/p/vector-search-duckdb-fastembed/>Vector Indexing and Search with DuckDB & FastEmbed</a></div></div></div><div class=single-pagination-prev><div class=single-pagination-container-prev><div class=single-pagination-text><a href=/p/larger-than-mem-intro/>Larger-Than-Memory Data Management</a></div><div class=single-pagination-text>→</div></div></div></div><hr></div><div class=back-to-top><a href=#top>back to top</a></div></div></main></div><footer><p>&mldr;</p></footer><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script></body><script>function isAuto(){return document.body.classList.contains("auto")}function setTheme(){if(!isAuto())return;document.body.classList.remove("auto");let e="light";window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches&&(e="dark"),document.body.classList.add(e)}function invertBody(){document.body.classList.toggle("dark"),document.body.classList.toggle("light")}isAuto()&&window.matchMedia("(prefers-color-scheme: dark)").addListener(invertBody),setTheme()</script></html>